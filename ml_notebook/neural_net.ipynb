{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jama2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import layers, initializers\n",
    "from sklearn import metrics\n",
    "\n",
    "from data_processing.parse_data import get_tracks_data\n",
    "from data_processing.cluster_data import create_clusters\n",
    "from analyse import validation\n",
    "from data_processing.parse_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'params_of_tracks_candidates.txt', sep=\", \", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          1\n",
       "1          0\n",
       "2          0\n",
       "3          1\n",
       "4          0\n",
       "          ..\n",
       "5989402    1\n",
       "5989403    0\n",
       "5989404    0\n",
       "5989405    1\n",
       "5989406    0\n",
       "Name: good, Length: 5989407, dtype: int8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['good'] = df['duplicate'] == df['fake']\n",
    "df['good'].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = df['prototrackIndex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_number = df['#format: eventNumber']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels=['#format: eventNumber', 'prototrackIndex'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nMeasurements', 'nStates', 'nOutliers', 'nHoles', 'nSharedHits',\n",
       "       'chi2PerNDF', 'pT', 'eta', 'duplicate', 'fake', 'good'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['good']\n",
    "y = y.astype('float32')\n",
    "X = df.iloc[:, :-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5989407, 8)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class margin_ranking_loss(tf.keras.losses.Loss):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "  def call(self, y_true, y_pred):\n",
    "    margin = 0.05\n",
    "    return tf.reduce_mean((tf.maximum(0.0, y_true - y_pred + margin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=len(X[0, :]), activation='relu',\n",
    "                    kernel_initializer='random_normal'))\n",
    "    model.add(Dense(10,activation='relu',kernel_initializer='random_normal'))\n",
    "    model.add(Dense(15,activation='relu',kernel_initializer='random_normal'))\n",
    "    model.add(Dense(10,activation='relu',kernel_initializer='random_normal'))\n",
    "    model.add(Dense(1,activation='sigmoid',\n",
    "                    kernel_initializer='random_normal'))\n",
    "    model.compile(loss=margin_ranking_loss(),\n",
    "                optimizer = 'adam',\n",
    "                metrics =['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"cp.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\n",
      "Epoch 1: saving model to cp.ckpt\n",
      "70189/70189 - 101s - loss: 0.0169 - accuracy: 0.3324 - val_loss: 0.0166 - val_accuracy: 0.3326 - 101s/epoch - 1ms/step\n",
      "Epoch 2/5\n",
      "\n",
      "Epoch 2: saving model to cp.ckpt\n",
      "70189/70189 - 110s - loss: 0.0166 - accuracy: 0.3324 - val_loss: 0.0166 - val_accuracy: 0.3326 - 110s/epoch - 2ms/step\n",
      "Epoch 3/5\n",
      "\n",
      "Epoch 3: saving model to cp.ckpt\n",
      "70189/70189 - 109s - loss: 0.0166 - accuracy: 0.3324 - val_loss: 0.0166 - val_accuracy: 0.3326 - 109s/epoch - 2ms/step\n",
      "Epoch 4/5\n",
      "\n",
      "Epoch 4: saving model to cp.ckpt\n",
      "70189/70189 - 104s - loss: 0.0166 - accuracy: 0.3324 - val_loss: 0.0166 - val_accuracy: 0.3326 - 104s/epoch - 1ms/step\n",
      "Epoch 5/5\n",
      "\n",
      "Epoch 5: saving model to cp.ckpt\n",
      "70189/70189 - 101s - loss: 0.0166 - accuracy: 0.3324 - val_loss: 0.0166 - val_accuracy: 0.3326 - 101s/epoch - 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21284af2770>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, validation_split = 0.25, batch_size =64,\n",
    "          callbacks=[cp_callback],verbose=2,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x21284a83670>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187169/187169 - 169s - loss: 0.0166 - accuracy: 0.3325 - 169s/epoch - 905us/step\n",
      "Restored model, accuracy: 33.25%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X, y, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(X)\n",
    "preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "6840/6840 - 63s - loss: 1.0065 - accuracy: 0.9671 - val_loss: 1.0000 - val_accuracy: 0.9667 - 63s/epoch - 9ms/step\n",
      "Epoch 2/1000\n",
      "6840/6840 - 56s - loss: 1.0000 - accuracy: 0.9671 - val_loss: 1.0000 - val_accuracy: 0.9667 - 56s/epoch - 8ms/step\n",
      "Epoch 3/1000\n",
      "6840/6840 - 53s - loss: 1.0000 - accuracy: 0.9671 - val_loss: 1.0000 - val_accuracy: 0.9667 - 53s/epoch - 8ms/step\n",
      "Epoch 4/1000\n",
      "6840/6840 - 51s - loss: 1.0000 - accuracy: 0.9671 - val_loss: 1.0000 - val_accuracy: 0.9667 - 51s/epoch - 7ms/step\n",
      "Epoch 5/1000\n",
      "6840/6840 - 52s - loss: 1.0000 - accuracy: 0.9671 - val_loss: 1.0000 - val_accuracy: 0.9667 - 52s/epoch - 8ms/step\n",
      "Epoch 6/1000\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "6840/6840 - 49s - loss: 1.0000 - accuracy: 0.9671 - val_loss: 1.0000 - val_accuracy: 0.9667 - 49s/epoch - 7ms/step\n",
      "Epoch 6: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2361906b910>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=len(X[0, :]), activation='relu',\n",
    "                kernel_initializer='random_normal'))\n",
    "model.add(Dense(10,activation='relu',kernel_initializer='random_normal'))\n",
    "model.add(Dense(15,activation='relu',kernel_initializer='random_normal'))\n",
    "model.add(Dense(10,activation='relu',kernel_initializer='random_normal'))\n",
    "model.add(Dense(1,activation='sigmoid',\n",
    "                kernel_initializer='random_normal'))\n",
    "model.compile(loss=margin_ranking_loss(),\n",
    "              optimizer=tensorflow.keras.optimizers.Adam(),\n",
    "              metrics =['accuracy'])\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5,\n",
    "                        verbose=1, mode='auto', restore_best_weights=True)\n",
    "\n",
    "model.fit(X, y, validation_split = 0.25, batch_size =64,\n",
    "          callbacks=[monitor],verbose=2,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: nn-model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: nn-model\\assets\n"
     ]
    }
   ],
   "source": [
    "# to save a model\n",
    "\n",
    "model.save('nn-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17099/17099 [==============================] - 105s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547156, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.DataFrame(columns=['track_id', 'pred'])\n",
    "df_preds['event'] = event_number\n",
    "df_preds['track_id'] = indexes\n",
    "df_preds['pred'] = preds\n",
    "df_preds['new'] = df_preds.apply(lambda x: str(int(x.event)) + '/' + str(int(x.track_id)) , axis=1)\n",
    "dct_preds = dict(zip(df_preds['new'], df_preds['pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('predictions.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(dct_preds, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_event(event_num, dct_preds, hits=3):\n",
    "    track_list = get_tracks_data(f\"../data/event_{event_num}_prototracks.txt\", f\"../data/event_{event_num}_space_points.txt\", True)\n",
    "    clusters = create_clusters(track_list, min_n_shared_hits=hits)\n",
    "    event_curr_good = []\n",
    "    for cluster in clusters:\n",
    "        max_score = 0\n",
    "        best_track = None\n",
    "        for dct in cluster:\n",
    "            track_id = [*dct][0]\n",
    "            hits = dct[track_id]\n",
    "            if dct_preds[f'{event_num}/{track_id}'] > max_score:\n",
    "                max_score = dct_preds[f'{event_num}/{track_id}']\n",
    "                best_track = hits\n",
    "        event_curr_good.append(best_track)\n",
    "    track_dict = get_hits_data(f\"../data/event_{event_num}_space_points.txt\")\n",
    "    hit_list = get_hits_data_for_validation(f\"../data/event_{event_num}_space_points.txt\")\n",
    "    track_id_dict = get_track_id(f\"../data/event_{event_num}_trackIds.txt\")\n",
    "\n",
    "    characteristic_dict = validation.calc_characteristics(event_curr_good, hit_list, track_dict, track_id_dict)\n",
    "    \n",
    "    return characteristic_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'efficiency': 0.9230769230769231, 'fake_rate': 0.0, 'duplication_rate': 0.0, 'purity': 0.39473684210526316, 'num_recognize_track': 12, 'num_real_track': 13, 'num_duplicate_track': 0, 'num_proto_track': 38, 'num_fake_track': 0, 'num_reco_dupl_track': 15}\n"
     ]
    }
   ],
   "source": [
    "print(process_event(3, dct_preds, hits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_1 = pd.DataFrame(columns=['efficiency', 'fake_rate', 'duplication_rate', 'purity',\n",
    "                                    'num_recognize_track', 'num_real_track', 'num_duplicate_track',\n",
    "                                    'num_proto_track', 'num_fake_track', 'num_reco_dupl_track'])\n",
    "df_result_3 = pd.DataFrame(columns=['efficiency', 'fake_rate', 'duplication_rate', 'purity',\n",
    "                                    'num_recognize_track', 'num_real_track', 'num_duplicate_track',\n",
    "                                    'num_proto_track', 'num_fake_track', 'num_reco_dupl_track'])\n",
    "df_result_5 = pd.DataFrame(columns=['efficiency', 'fake_rate', 'duplication_rate', 'purity',\n",
    "                                    'num_recognize_track', 'num_real_track', 'num_duplicate_track',\n",
    "                                    'num_proto_track', 'num_fake_track', 'num_reco_dupl_track'])\n",
    "df_result_7 = pd.DataFrame(columns=['efficiency', 'fake_rate', 'duplication_rate', 'purity',\n",
    "                                    'num_recognize_track', 'num_real_track', 'num_duplicate_track',\n",
    "                                    'num_proto_track', 'num_fake_track', 'num_reco_dupl_track'])\n",
    "df_result_9 = pd.DataFrame(columns=['efficiency', 'fake_rate', 'duplication_rate', 'purity',\n",
    "                                    'num_recognize_track', 'num_real_track', 'num_duplicate_track',\n",
    "                                    'num_proto_track', 'num_fake_track', 'num_reco_dupl_track'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------0------\n",
      "------1------\n",
      "------2------\n",
      "------3------\n",
      "------4------\n",
      "------5------\n",
      "------6------\n",
      "------7------\n",
      "------8------\n",
      "------9------\n",
      "------10------\n",
      "------11------\n",
      "------12------\n",
      "------13------\n",
      "------14------\n",
      "------15------\n",
      "------16------\n",
      "------17------\n",
      "------18------\n",
      "------19------\n",
      "------20------\n",
      "------21------\n",
      "------22------\n",
      "------23------\n",
      "------24------\n",
      "------25------\n",
      "------26------\n",
      "------27------\n",
      "------28------\n",
      "------29------\n",
      "------30------\n",
      "------31------\n",
      "------32------\n",
      "------33------\n",
      "------34------\n",
      "------35------\n",
      "------36------\n",
      "------37------\n",
      "------38------\n",
      "------39------\n",
      "------40------\n",
      "------41------\n",
      "------42------\n",
      "------43------\n",
      "------44------\n",
      "------45------\n",
      "------46------\n",
      "------47------\n",
      "------48------\n",
      "------49------\n",
      "------50------\n",
      "------51------\n",
      "------52------\n",
      "------53------\n",
      "------54------\n",
      "------55------\n",
      "------56------\n",
      "------57------\n",
      "------58------\n",
      "------59------\n",
      "------60------\n",
      "------61------\n",
      "------62------\n",
      "------63------\n",
      "------64------\n",
      "------65------\n",
      "------66------\n",
      "------67------\n",
      "------68------\n",
      "------69------\n",
      "------70------\n",
      "------71------\n",
      "------72------\n",
      "------73------\n",
      "------74------\n",
      "------75------\n",
      "------76------\n",
      "------77------\n",
      "------78------\n",
      "------79------\n",
      "------80------\n",
      "------81------\n",
      "------82------\n",
      "------83------\n",
      "------84------\n",
      "------85------\n",
      "------86------\n",
      "------87------\n",
      "------88------\n",
      "------89------\n",
      "------90------\n",
      "------91------\n",
      "------92------\n",
      "------93------\n",
      "------94------\n",
      "------95------\n",
      "------96------\n",
      "------97------\n",
      "------98------\n",
      "------99------\n",
      "------100------\n"
     ]
    }
   ],
   "source": [
    "for i in range(101):\n",
    "    print(f'------{i}------')\n",
    "    for df, hit in zip([df_result_1, \n",
    "                        df_result_3,\n",
    "                        df_result_5,\n",
    "                        df_result_7,\n",
    "                        df_result_9],\n",
    "                        [1, 3, 5, 7, 9]):\n",
    "        dct = process_event(i, dct_preds, hit)\n",
    "        eff = dct['efficiency']\n",
    "        fake = dct['fake_rate']\n",
    "        dup = dct['duplication_rate']\n",
    "        pur = dct['purity']\n",
    "        reco = dct['num_recognize_track']\n",
    "        real = dct['num_real_track']\n",
    "        dupl = dct['num_duplicate_track']\n",
    "        proto = dct['num_proto_track']\n",
    "        fake = dct['num_fake_track']\n",
    "        reco_dupl = dct['num_reco_dupl_track']\n",
    "        \n",
    "        df.loc[i] = [eff, fake, dup, pur, reco, real, dupl, proto, fake, reco_dupl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, hit in zip([df_result_1, df_result_3,\n",
    "                    df_result_5, df_result_7, df_result_9],\n",
    "                    [1, 3, 5, 7, 9]):\n",
    "        df.to_csv(f'result_{hit}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
